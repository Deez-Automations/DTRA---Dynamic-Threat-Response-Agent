# -*- coding: utf-8 -*-
"""Dynamic Threat Response Agent

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XDReOIhiOZwpO9qzoPcUsZLZ0waVgSWr
"""

from google.colab import drive
drive.mount('/content/drive')

"""**REPORT I: DETECTOR USING LOGISTIC REGRESSION**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from google.colab import drive
import os

# --- Define Your EXACT Features (The "Clue List") ---
# This list is based on the 79 clues you found.
# 'Label' is left out, as it's the "answer".
numeric_features = [
    'Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',
    'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max',
    'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std',
    'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean',
    'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean',
    'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean',
    'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',
    'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',
    'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',
    'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length',
    'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',
    'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count',
    'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count',
    'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',
    'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',
    'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk',
    'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',
    'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes',
    'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd',
    'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min',
    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'
]

# --- 2. Load and Prepare Data ---
print("--- 1. Loading and Preparing Data... ---")
FILE_PATH = '/content/drive/MyDrive/CICIDS2017/COMBINED_CICIDS2017.csv'

try:
    df = pd.read_csv(FILE_PATH, encoding='latin1', low_memory=False)
except FileNotFoundError:
    print(f"ERROR: File not found at {FILE_PATH}")
    print("Please make sure you ran the 'combine_files_colab.py' script first.")
    raise

# Clean column names (strip spaces)
df.columns = df.columns.str.strip()

# ** This is the only column we need to use from our list **
# We will tell pandas to *only* load these columns + the Label
# This is faster and safer.
columns_to_load = numeric_features + ['Label']
df = df[columns_to_load]

# Sample the data (100,000 rows)
print(f"Original data shape (after selecting columns): {df.shape}")
df = df.sample(n=100000, random_state=42)
print(f"Sampled data shape: {df.shape}")

# Clean numeric data (Infinity, NaN)
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Clean labels (0 for BENIGN, 1 for anything else)
df['label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)
df = df.drop('Label', axis=1)

print("--- Data Loading and Cleaning Complete ---")

# --- 3. Define X (features) and y (answer) ---
# X is all our numeric "clues"
X = df[numeric_features]
y = df['label']


# --- 4. Build Preprocessing Pipeline ("Automatic Cleaner") ---
# This is now *simpler*! We only need the numeric cleaner.
# No ColumnTransformer is needed.
preprocessor = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
print("--- 2. Built the *simplified* 'Data Preprocessing Pipeline' ---")


# --- 5. Build Full Pipeline (Cleaner + "Detector") ---
full_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000, solver='saga'))
])

# --- 6. Train the "Detector" ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print("--- 3. Training the 'Detector' (this may take a minute)... ---")
full_pipeline.fit(X_train, y_train)
print("Training complete!")

# --- 7. Evaluate the "Detector" (Get Grades) ---
print("--- 4. Evaluating the 'Detector'... ---")
y_pred = full_pipeline.predict(X_test)
y_pred_probs = full_pipeline.predict_proba(X_test)
confidence_scores = y_pred_probs[:, 1] # "Danger Score"

print(f"\n--- A. Example 'Confidence Scores' (first 5 test samples) ---")
for i in range(5):
    print(f"  Sample {i+1}: 'Danger Score' = {confidence_scores[i]:.4f} ({(confidence_scores[i]*100):.1f}%)")

print("\n--- B. 'The Report Card' (Classification Report) ---")
print(classification_report(y_test, y_pred, zero_division=0))

print("\n--- C. 'The Alert Fatigue' Score (Confusion Matrix) ---")
cm = confusion_matrix(y_test, y_pred)
print("          [Predicted Leaf] [Predicted Robber]")
print(f"Actual Leaf:    [   {cm[0][0]:<6}   ]      [   {cm[0][1]:<6}   ]  <-- False Positive (Alert Fatigue)!")
print(f"Actual Robber:  [   {cm[1][0]:<6}   ]      [   {cm[1][1]:<6}   ]  <-- False Negative (Missed Attack)!")
print("\n")

"""**REPORT II: DECIDER : A^ ALGORITHM**"""

# ---  A* "Decider" Agent ---
BUSINESS_COSTS = {
    'Ignore': 0,    # Free
    'Log': 2,       # Very cheap
    'Block': 20,    # Disruptive
    'Isolate': 80   # Very disruptive
}

# --- 2. Define "h(n)" (Security Risk) ---
# This calculates the security risk based on the "Danger Score".
def calculate_security_risk(action, danger_score):
    if action == 'Ignore':
        # Risk of ignoring is the full danger score.
        return danger_score * 100

    if action == 'Log':
        # Risk of logging is slightly less than ignoring.
        return danger_score * 90

    if action == 'Block':
        # Risk of blocking is the "uncertainty" (risk of a false positive).
        return (1.0 - danger_score) * 20

    if action == 'Isolate':
        # Risk of isolating is 0; the threat is contained.
        return 0

# --- 3. Define the A* "Decider" Agent ---
def find_best_action(danger_score):
    # Loops through all actions and finds the one with the lowest "Total Cost".
    print(f"--- 'Decider' is thinking about a {danger_score*100:.1f}% Danger Score ---")

    results = [] # Will hold (total_cost, action)

    for action, g_cost in BUSINESS_COSTS.items():
        h_cost = calculate_security_risk(action, danger_score)
        f_cost = g_cost + h_cost

        print(f"  - Action: {action:<8} | Total Cost: {f_cost:>6.2f} (g_cost={g_cost:<2} + h_cost={h_cost:<6.2f})")
        results.append((f_cost, action))

    # Find the "cheapest" (best) action
    best_cost, best_action = min(results)

    return best_action, best_cost


# --- 4. The "Working Example" ---
# This runs three tests to demonstrate the agent's logic.
if __name__ == "__main__":

    print("--- Running A* 'Decider' Agent: Working Examples ---")

    # Test 1: The "Leaf" (1% Danger)
    leaf_score = 0.01
    leaf_action, leaf_cost = find_best_action(leaf_score)
    print(f"  ==> Best Action for 'Leaf': {leaf_action} (Total Cost: {leaf_cost:.2f})\n")

    # Test 2: The "Robber" (95% Danger)
    robber_score = 0.95
    robber_action, robber_cost = find_best_action(robber_score)
    print(f"  ==> Best Action for 'Robber': {robber_action} (Total Cost: {robber_cost:.2f})\n")

    # Test 3: The "Ambiguous" Threat (60% Danger)
    ambiguous_score = 0.60
    amb_action, amb_cost = find_best_action(ambiguous_score)
    print(f"  ==> Best Action for 'Ambiguous': {amb_action} (Total Cost: {amb_cost:.2f})\n")

"""**REPORT III : PART 1-> IMPROVING DETECTOR TO DEEP NEURAL NETWORKS**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from google.colab import drive
import os

# --- Define Features (Identical to Report II) ---
numeric_features = [
    'Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',
    'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max',
    'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std',
    'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean',
    'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean',
    'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean',
    'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',
    'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',
    'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',
    'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length',
    'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',
    'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count',
    'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count',
    'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',
    'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',
    'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk',
    'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',
    'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes',
    'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd',
    'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min',
    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'
]
# We only use numeric features, so the preprocessor is simpler.

# --- 1. Load Data ---
if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')
print("--- 1. Loading and Preparing Data... ---")
FILE_PATH = '/content/drive/MyDrive/CICIDS2017/COMBINED_CICIDS2017.csv'

try:
    df = pd.read_csv(FILE_PATH, encoding='latin1', low_memory=False)
except FileNotFoundError:
    print("ERROR: File not found. Check path.")
    raise

# Data Cleaning Steps
df.columns = df.columns.str.strip()
columns_to_load = numeric_features + ['Label']
df = df[columns_to_load]
df = df.sample(n=1000000, random_state=42) # Sample 100k rows
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Label Encoding (0=Benign, 1=Attack)
df['label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)
df = df.drop('Label', axis=1)

print(f"Sampled data shape: {df.shape}")

# Define X and y
X = df[numeric_features]
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# --- 2. Advanced Preprocessing (For Neural Network) ---
# We fit the Scaler/Imputer manually on X_train before passing to the DNN
print("--- 2. Building Advanced Preprocessor... ---")
imputer = SimpleImputer(strategy='median')
scaler = StandardScaler()

# Impute NaNs using the median from the training set
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Scale the data
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

# Get the number of input features (79)
input_dim = X_train_scaled.shape[1]


# --- 3. Build the Deep Neural Network (DNN) Model ---
# This is our "Advanced ML" model, fulfilling Section 7
print("--- 3. Building and Training DNN Model (T4 GPU recommended)... ---")
model = Sequential([
    # Input Layer (Input is the 79 features)
    Dense(128, activation='relu', input_shape=(input_dim,)),
    Dropout(0.3), # Prevents overfitting

    # Hidden Layer 1
    Dense(64, activation='relu'),
    Dropout(0.3),

    # Output Layer (1 node for the probability/Danger Score)
    # 'sigmoid' activation is perfect for giving a probability (0.0 to 1.0)
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Early stopping prevents running too long if the model stops improving
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(X_train_scaled, y_train,
                    epochs=50, # We give it 50 chances to learn
                    batch_size=32,
                    validation_split=0.1, # Use 10% of training data for internal validation
                    callbacks=[early_stop],
                    verbose=1)

print("Training complete.")


# --- 4. Evaluate and Get Metrics ---
# We now evaluate the new model's performance on the test set
print("--- 4. Evaluating Advanced Model Performance: ---")

# Get predictions (0 or 1)
y_pred_dnn = (model.predict(X_test_scaled) > 0.5).astype("int32")

# Get probabilities (Danger Scores)
y_pred_probs_dnn = model.predict(X_test_scaled)
confidence_scores = y_pred_probs_dnn[:, 0]

# --- Print Results ---
print("\n--- A. Example 'Confidence Scores' (first 5 test samples) ---")
for i in range(5):
    print(f"  Sample {i+1}: 'Danger Score' = {confidence_scores[i]:.4f} ({(confidence_scores[i]*100):.1f}%)")

print("\n--- B. 'The Report Card' (Classification Report) ---")
# Use the predictions (y_pred_dnn) to generate the report
print(classification_report(y_test, y_pred_dnn, zero_division=0))

print("\n--- C. 'The Alert Fatigue' Score (Confusion Matrix) ---")
cm_dnn = confusion_matrix(y_test, y_pred_dnn)
print("          [Predicted Leaf] [Predicted Robber]")
print(f"Actual Leaf:    [   {cm_dnn[0][0]:<6}   ]      [   {cm_dnn[0][1]:<6}   ]  <-- False Positive (Alert Fatigue)!")
print(f"Actual Robber:  [   {cm_dnn[1][0]:<6}   ]      [   {cm_dnn[1][1]:<6}   ]  <-- False Negative (Missed Attack)!")
print("\n")

"""**REPORT III: PART 2-> Interpretability**"""

import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from google.colab import drive
import os
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# --- IMPORTANT: Install SHAP if you haven't yet ---
# Run this command in a separate cell FIRST if you get a "ModuleNotFoundError"
# !pip install shap

# --- Define Features (The 79 Confirmed Columns) ---
all_expected_columns_from_file = [
    'Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',
    'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max',
    'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std',
    'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean',
    'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean',
    'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean',
    'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',
    'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',
    'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',
    'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length',
    'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',
    'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count',
    'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count',
    'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',
    'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',
    'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk',
    'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',
    'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes',
    'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd',
    'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min',
    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label'
]

numeric_features = [col for col in all_expected_columns_from_file if col != 'Label']

# --- 1. Data Setup ---
if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')

print("=" * 60)
print("SHAP INTERPRETABILITY ANALYSIS")
print("=" * 60)
print("\n[1/4] Loading and preprocessing data...")

FILE_PATH = '/content/drive/MyDrive/CICIDS2017/COMBINED_CICIDS2017.csv'

try:
    df = pd.read_csv(FILE_PATH, encoding='latin1', low_memory=False)
except FileNotFoundError:
    print("ERROR: File not found. Check path.")
    raise

df.columns = df.columns.str.strip()
df = df[all_expected_columns_from_file]
df = df.sample(n=1000000, random_state=42)
df.replace([np.inf, -np.inf], np.nan, inplace=True)

df['label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)
df = df.drop('Label', axis=1)

print(f"‚úì Data loaded successfully: {df.shape[0]:,} samples")

X = df[numeric_features]
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- Apply Preprocessing ---
imputer = SimpleImputer(strategy='median')
scaler = StandardScaler()
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

kept_indices = np.where(scaler.var_ > 1e-6)[0]
final_feature_names = [numeric_features[i] for i in kept_indices]
actual_input_dim = len(final_feature_names)

X_test_scaled_df = pd.DataFrame(X_test_scaled[:, kept_indices], columns=final_feature_names)
X_train_scaled_df = pd.DataFrame(X_train_scaled[:, kept_indices], columns=final_feature_names)

print(f"‚úì Preprocessing complete: {actual_input_dim} features retained")

# --- 2. Build Model ---
print("\n[2/4] Building neural network model...")

model = Sequential([
    Dense(128, activation='relu', input_shape=(actual_input_dim,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy')

print("‚úì Model structure created")

# --- 3. SHAP Explainer ---
print("\n[3/4] Computing SHAP values (this may take a few minutes)...")

background = X_train_scaled_df.sample(100, random_state=42)
explainer = shap.DeepExplainer(model, background.values)
shap_values = explainer.shap_values(X_test_scaled_df.head(100).values)

if isinstance(shap_values, list):
    shap_values = shap_values[0]

while len(shap_values.shape) > 2:
    shap_values = shap_values.squeeze(axis=-1)

print("‚úì SHAP analysis complete")

# --- 4. Visualize Results ---
print("\n[4/4] Generating visualizations...")
print("\n" + "=" * 60)
print("RESULTS: GLOBAL FEATURE IMPORTANCE")
print("=" * 60 + "\n")

shap.summary_plot(shap_values, X_test_scaled_df.head(100), show=False)
plt.tight_layout()
plt.show()

print("\nInterpretation:")
print("‚Ä¢ Features at the top have the strongest overall influence on predictions")
print("‚Ä¢ Red points = High feature values pushing toward 'Attack' prediction")
print("‚Ä¢ Blue points = Low feature values pushing toward 'Benign' prediction")

# --- Individual Alert Explanation ---
attack_indices = np.where(y_test.head(100).values == 1)[0]

if len(attack_indices) == 0:
    print("\n‚ö† No attack samples found in the first 100 test samples.")
else:
    attack_index = attack_indices[0]

    print("\n" + "=" * 60)
    print(f"INDIVIDUAL ALERT EXPLANATION: Alert #{attack_index}")
    print("=" * 60 + "\n")

    feature_importance = pd.DataFrame({
        'feature': X_test_scaled_df.columns,
        'shap_value': shap_values[attack_index]
    }).sort_values('shap_value', key=abs, ascending=False).head(10)

    plt.figure(figsize=(10, 6))
    colors = ['#FF4444' if x > 0 else '#4444FF' for x in feature_importance['shap_value']]
    plt.barh(feature_importance['feature'], feature_importance['shap_value'], color=colors)
    plt.xlabel('SHAP Value (Impact on Prediction)', fontsize=12)
    plt.title(f'Top 10 Most Influential Features for Alert #{attack_index}', fontsize=14, fontweight='bold')
    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
    plt.tight_layout()
    plt.show()

    print("Interpretation:")
    print("‚Ä¢ Red bars = Features pushing prediction toward 'Attack'")
    print("‚Ä¢ Blue bars = Features pushing prediction toward 'Benign'")

    abs_shap_values = np.abs(shap_values[attack_index])
    top_indices = np.argsort(abs_shap_values)[::-1][:5]

    print(f"\nüìä Top 3 Most Influential Features:\n")

    for i, index in enumerate(top_indices[:3]):
        feature_name = X_test_scaled_df.columns[index]
        feature_value = X_test_scaled_df.iloc[attack_index, index]

        dummy_array_78 = np.zeros((1, 78))
        original_feature_index = kept_indices[index]
        dummy_array_78[0, original_feature_index] = feature_value

        original_value_array = scaler.inverse_transform(dummy_array_78)
        original_value = original_value_array[0, original_feature_index]

        shap_contribution = shap_values[attack_index][index]
        direction = "Attack üî¥" if shap_contribution > 0 else "Benign üîµ"

        print(f"  {i+1}. {feature_name}")
        print(f"     Value: {original_value:,.2f}")
        print(f"     Impact: Pushed prediction toward {direction}\n")

print("\n" + "=" * 60)
print("‚úì SHAP Interpretability Analysis Complete")
print("=" * 60)

"""**Reinforcement Learning (Q-Learning) - The Adaptive Decider**"""

import numpy as np
import random
import matplotlib.pyplot as plt

# --- 1. Define the RL Environment and Parameters ---

# States: Discretize the 0-100% Danger Score into 10 levels
# State 0 = 0-10% Danger (Leaf)
# State 9 = 90-100% Danger (Robber)
NUM_STATES = 10

# Actions: The four responses available
ACTIONS = ['Ignore', 'Log', 'Block', 'Isolate']
NUM_ACTIONS = len(ACTIONS)

# Q-Learning Parameters
ALPHA = 0.1     # Learning rate (how quickly the agent updates its beliefs)
GAMMA = 0.9     # Discount factor (how much the agent values future rewards)
EPSILON = 0.1   # Exploration rate (how often the agent takes a random action)
NUM_EPISODES = 10000 # Number of simulated events (training runs)

# --- 2. Initialize the Q-Table ---
# The Q-Table is the agent's brain: [State x Action]
# It stores the expected long-term reward for every possible move.
q_table = np.zeros((NUM_STATES, NUM_ACTIONS))


# --- 3. Define the Reward Function (The Rules of the SOC) ---
def get_reward(state, action_index):
    """
    Returns the immediate reward for taking an action in a given state.
    """
    action = ACTIONS[action_index]
    danger_level = state + 1 # Converts state index (0-9) to 1-10

    # Low Danger (Leafs): States 0-2 (10% to 30% Danger)
    if danger_level <= 3:
        if action == 'Ignore' or action == 'Log':
            return 0  # Correct filtration: Neutral reward
        else: # Block/Isolate on a Leaf
            return -20 # Penalty for False Positive (Alert Fatigue)

    # Medium Danger: States 3-6 (40% to 70% Danger)
    elif danger_level <= 7:
        if action == 'Block':
            return 10 # Good decision, but risky
        elif action == 'Isolate':
            return -50 # Penalty: Too disruptive for medium risk
        else:
            return -10 # Penalty: Ignoring a 50% chance is risky

    # High Danger (Robbers): States 7-9 (80% to 100% Danger)
    else:
        if action == 'Block' or action == 'Isolate':
            return 50  # High Reward for correct autonomous response
        elif action == 'Ignore':
            return -100 # Maximum Penalty for Missed Attack

    return -1 # Default penalty for invalid/unspecified move


# --- 4. The Q-Learning Training Algorithm ---
reward_history = []

print("--- 1. Training Q-Learning Agent (10,000 Simulated Events) ---")

for episode in range(NUM_EPISODES):
    # Start of a new simulation (new alert arrives)

    # Randomly generate an alert State (e.g., State 5, 50% danger)
    state = random.randint(0, NUM_STATES - 1)

    # --- 4a. Epsilon-Greedy Strategy (Explore or Exploit) ---
    if random.uniform(0, 1) < EPSILON:
        # Explore: Take a random action (try something new)
        action = random.randint(0, NUM_ACTIONS - 1)
    else:
        # Exploit: Take the best action found so far (based on Q-Table)
        action = np.argmax(q_table[state, :])

    # --- 4b. Receive Reward and Update Q-Table ---
    reward = get_reward(state, action)

    # Q-Learning Formula:
    # New Q(s, a) = (1-Œ±) * Q(s, a) + Œ± * [ R + Œ≥ * max Q(s', a') ]
    # The 'next state' is just the current state because our environment is simple (S=S')
    old_q_value = q_table[state, action]
    new_q_value = old_q_value + ALPHA * (reward + GAMMA * np.max(q_table[state, :]) - old_q_value)

    # Update the Q-Table "Brain"
    q_table[state, action] = new_q_value

    reward_history.append(reward)

print("Training complete.")


# --- 5. Analysis and Results ---
print("\n--- 2. Q-Learning Analysis (Optimal Policy) ---")

# Print the final learned policy (the agent's definitive rulebook)
optimal_policy = {}
for state in range(NUM_STATES):
    best_action_index = np.argmax(q_table[state, :])
    optimal_policy[f'{state*10}%-{state*10+10}% Danger'] = ACTIONS[best_action_index]

print("\n--- Final Learned Optimal Policy ---")
print("  (This shows the best action the agent LEARNED for each Danger Level)")
for danger_range, action in optimal_policy.items():
    print(f"  {danger_range:<20} --> {action}")


# Plot the cumulative reward to show the agent learned
cumulative_rewards = np.cumsum(reward_history)
episodes = np.arange(1, NUM_EPISODES + 1)
window_size = 500
smoothed_rewards = np.convolve(cumulative_rewards, np.ones(window_size)/window_size, mode='valid')

plt.figure(figsize=(10, 5))
plt.plot(episodes[window_size-1:], smoothed_rewards, label='Smoothed Cumulative Reward')
plt.title('RL Agent Learning Progress (Reward Convergence)')
plt.xlabel('Episode (Simulated Event)')
plt.ylabel('Cumulative Reward')
plt.grid(True)
plt.show()

print("\n--- RL Agent Prototype Complete. ---")

"""**STREAMLIT FOR DASBOARD**

**SAVING MODELS FOR MAIN CNS**
"""

import joblib
import numpy as np
import os

# 1. Save the Detector Brain (DNN)
# We save the Keras model to an .h5 file
model.save('dtra_detector_model.h5')
print("‚úì Saved Detector Model (dtra_detector_model.h5)")

# 2. Save the Math Rules (Preprocessing)
# We use joblib to save the fitted Imputer and Scaler
joblib.dump(imputer, 'dtra_imputer.pkl')
joblib.dump(scaler, 'dtra_scaler.pkl')
print("‚úì Saved Preprocessing Rules (dtra_imputer.pkl, dtra_scaler.pkl)")

# 3. Save the Decider Brain (RL Agent)
# We save the Q-Table numpy array
np.save('dtra_q_table.npy', q_table)
print("‚úì Saved RL Q-Table (dtra_q_table.npy)")

"""**THE MAIN CNS**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import joblib
# import time
# import plotly.graph_objects as go
# import plotly.express as px
# from tensorflow.keras.models import load_model
# import tensorflow as tf
# import os
# 
# # --- 1. PERFORMANCE CONFIGURATION ---
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
# 
# st.set_page_config(
#     page_title="DTRA | Enterprise Sentinel",
#     page_icon="üõ°Ô∏è",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# # Custom CSS
# st.markdown("""
# <style>
#     .stApp { background-color: #000000; color: #00FF41; font-family: 'Courier New', monospace; }
#     .stMetric { background-color: #0A0A0A; border: 1px solid #1F1F1F; padding: 15px; border-radius: 4px; }
#     .metric-value { color: #00FF41 !important; }
#     h1, h2, h3 { color: #E0E0E0; }
# </style>
# """, unsafe_allow_html=True)
# 
# # --- 2. LOAD BRAINS ---
# @st.cache_resource
# def load_core():
#     try:
#         model = load_model('dtra_detector_model.h5')
#         scaler = joblib.load('dtra_scaler.pkl')
#         imputer = joblib.load('dtra_imputer.pkl')
#         q_table = np.load('dtra_q_table.npy')
#         return model, scaler, imputer, q_table
#     except Exception as e:
#         return None, None, None, None
# 
# model, scaler, imputer, q_table = load_core()
# 
# # --- 3. INTELLIGENCE LAYER ---
# 
# def get_strategic_action(danger_score, q_table):
#     # Map score to bucket 0-9
#     state = min(int(danger_score * 10), 9)
#     action_idx = np.argmax(q_table[state, :])
#     return ['Ignore', 'Log', 'Block', 'Isolate'][action_idx]
# 
# @tf.function(reduce_retracing=True)
# def fast_inference(model, input_tensor):
#     return model(input_tensor, training=False)
# 
# def brain_process_packet(row_values, model, scaler, imputer):
#     """
#     ROBUST PROCESSING: Handles 78 vs 79 column mismatch automatically.
#     """
#     target_dim = 79
#     current_dim = len(row_values)
# 
#     # --- AUTO-ADAPTER ---
#     if current_dim < target_dim:
#         # Pad with zeros if columns are missing
#         row_values = np.pad(row_values, (0, target_dim - current_dim), 'constant')
#     elif current_dim > target_dim:
#         # Trim if too many
#         row_values = row_values[:target_dim]
# 
#     # Reshape
#     row_reshaped = row_values.reshape(1, -1)
# 
#     # --- PREPROCESSING ---
#     imputed_data = imputer.transform(row_reshaped)
#     scaled_data = scaler.transform(imputed_data)
# 
#     # --- INFERENCE ---
#     input_tensor = tf.convert_to_tensor(scaled_data, dtype=tf.float32)
#     prediction = fast_inference(model, input_tensor)
# 
#     return float(prediction[0][0])
# 
# # --- 4. DASHBOARD UI ---
# 
# st.sidebar.title("üéõÔ∏è OPS COMMAND")
# uploaded_file = st.sidebar.file_uploader("TRAFFIC INGEST (CSV)", type="csv")
# latency_control = st.sidebar.select_slider("CLOCK", options=["Real-Time", "Fast", "Super-Fast"], value="Fast")
# active_defense = st.sidebar.checkbox("ACTIVATE DEFENSE MATRIX", value=False)
# 
# st.title("DTRA SENTINEL")
# st.markdown("`STATUS: ONLINE` | `MODE: AUTONOMOUS`")
# 
# k1, k2, k3, k4 = st.columns(4)
# with k1: m_scan = st.empty()
# with k2: m_threat = st.empty()
# with k3: m_block = st.empty()
# with k4: m_noise = st.empty()
# 
# g1, g2 = st.columns([2, 1])
# with g1: chart_risk = st.empty()
# with g2: chart_action = st.empty()
# 
# st.markdown("### üõë EVENT LOG")
# log_feed = st.empty()
# 
# # --- 5. EXECUTION LOOP ---
# 
# if active_defense and uploaded_file and model:
#     try:
#         df_chunk = pd.read_csv(uploaded_file, encoding='latin1', nrows=1000)
#         df_numeric = df_chunk.select_dtypes(include=[np.number])
# 
#         total, threats, blocks, noise = 0, 0, 0, 0
#         risk_buffer = []
#         action_map = {'Ignore':0, 'Log':0, 'Block':0, 'Isolate':0}
#         logs = []
# 
#         sleep_time = 0.5 if latency_control == "Real-Time" else 0.05 if latency_control == "Fast" else 0.001
# 
#         for i in range(len(df_numeric)):
#             # Extract
#             raw_packet = df_numeric.iloc[i].values
# 
#             # Brain Processing (Now Crash-Proof)
#             risk_score = brain_process_packet(raw_packet, model, scaler, imputer)
#             decision = get_strategic_action(risk_score, q_table)
# 
#             # Update State
#             total += 1
#             if risk_score > 0.5: threats += 1
#             if decision in ['Block', 'Isolate']: blocks += 1
#             else: noise += 1
#             action_map[decision] += 1
# 
#             risk_buffer.append(risk_score)
#             if len(risk_buffer) > 100: risk_buffer.pop(0)
# 
#             # Render UI
#             m_scan.metric("PACKETS", f"{total:,}")
#             m_threat.metric("THREATS", f"{threats:,}", delta=f"{(threats/total)*100:.1f}%")
#             m_block.metric("BLOCKS", f"{blocks:,}")
#             m_noise.metric("FILTERED", f"{noise:,}")
# 
#             fig_risk = px.area(y=risk_buffer, height=220, title="THREAT PULSE")
#             fig_risk.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', font_color='#00FF41', margin=dict(l=0,r=0,t=30,b=0), yaxis=dict(range=[0, 1], showgrid=False), xaxis=dict(showgrid=False))
#             fig_risk.update_traces(line_color='#00FF41', fill_color='rgba(0, 255, 65, 0.1)')
#             chart_risk.plotly_chart(fig_risk, use_container_width=True)
# 
#             fig_pie = px.pie(values=list(action_map.values()), names=list(action_map.keys()), hole=0.6, title="DECISIONS")
#             fig_pie.update_layout(paper_bgcolor='rgba(0,0,0,0)', font_color='white', margin=dict(l=0,r=0,t=30,b=0), showlegend=False, height=220)
#             fig_pie.update_traces(textinfo='label+percent', marker=dict(colors=['#333', '#0044FF', '#FF5500', '#FF0000']))
#             chart_action.plotly_chart(fig_pie, use_container_width=True)
# 
#             status_icon = "üî¥" if risk_score > 0.8 else "üü°" if risk_score > 0.4 else "üü¢"
#             new_log = {"ID": f"PKT-{i}", "RISK": f"{risk_score:.4f}", "SEVERITY": status_icon, "DECISION": decision}
#             logs.insert(0, new_log)
#             if len(logs) > 5: logs.pop()
#             log_feed.dataframe(pd.DataFrame(logs), use_container_width=True)
# 
#             time.sleep(sleep_time)
# 
#     except Exception as e:
#         st.error(f"SYSTEM HALTED: {e}")
# elif not uploaded_file:
#     st.info("AWAITING DATA STREAM... UPLOAD CSV TO ACTIVATE.")

"""**CLOUDFARE APPROACH**

**Launching Dashboard**
"""

!pip install streamlit -q
!wget -q -O - ipv4.icanhazip.com

# 1. Kill any old, stuck tunnels
!fuser -k 8501/tcp

# 2. Download Cloudflare (if missing)
!wget -q -nc https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64
!chmod +x cloudflared-linux-amd64

# 3. Start Tunnel
print("Starting Tunnel...")
!nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &> nohup.out &

# 4. Start App
print("Starting Dashboard...")
!streamlit run app.py &>/content/logs.txt &

# 5. Get Link
import time
time.sleep(5)
print("\nüëá CLICK THIS LINK TO OPEN DASHBOARD üëá")
!grep -o 'https://.*\.trycloudflare.com' nohup.out | head -n 1